# -*- coding: utf-8 -*-
"""prapemrosesan_data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mkw6U8QEsIl1wEhTx-72qTbs_BLd-gXG
"""

# load ke dataset yang ada di drive

from google.colab import drive
drive.mount('/content/drive')

# inside a folder called 'datasets' in your Google Drive's 'My Drive'
import pandas as pd
df = pd.read_csv('/content/drive/My Drive/Tugas_akhir/DataTA(okky).csv', sep=';')
print(df.head())

# melakukan case folding pada data

# Apply case folding to all string columns
for col in df.columns:
    if df[col].dtype == 'object':  # Check if the column contains strings
        df[col] = df[col].str.lower()

# Save the modified Data
df.to_csv('/content/drive/My Drive/Tugas_akhir/DataTA(okky).csv', sep=';', index=False)
print(df.head())

import re
import pandas as pd

def clean_text(text):
    # Skip cleansing if value is NaN
    if pd.isna(text):
        return text
    # Remove URLs
    text = re.sub(r'http\S+', '', text)
    # Remove emoticons
    text = re.sub(r'[^\w\s,]', '', text)
    # Remove hashtags
    text = re.sub(r'#\w+', '', text)
    return text

# Apply cleansing to all string columns
for col in df.columns:
    if df[col].dtype == 'object':
        # Skip NaN values and apply cleansing
        df[col] = df[col].apply(clean_text)

# Save the modified Data
df.to_csv('/content/drive/My Drive/Tugas_akhir/DataTA(okky).csv', sep=';', index=False)

print(df.head())

import pandas as pd

# Baca file CSV dengan data utama
df = pd.read_csv('/content/drive/My Drive/Tugas_akhir/DataTA(okky).csv', sep=';')

# Baca file CSV kamus kata baku
kamus_df = pd.read_csv('/content/drive/My Drive/Tugas_akhir/new_kamusalay.csv', encoding='latin-1')

# Check the actual column names in your kamus_df
print(kamus_df.columns)

# Konversi kamus ke dictionary using the correct column names
# Replace 'slang' and 'formal' with the correct column names if they are different
normalization_dict = dict(zip(kamus_df['anakjakartaasikasik'], kamus_df['anak jakarta asyik asyik']))

# Fungsi untuk normalisasi kata berdasarkan kamus
def normalize_text(text):
    if pd.isna(text):
        return text
    for word, replacement in normalization_dict.items():
        text = text.replace(f" {word} ", f" {replacement} ")
    return text

# Terapkan normalisasi ke semua kolom string
for col in df.columns:
    if df[col].dtype == 'object':
        df[col] = df[col].apply(normalize_text)

# Simpan Data yang telah diubah
df.to_csv('/content/drive/My Drive/Tugas_akhir/DataTA(okky).csv', sep=';', index=False)

# Tampilkan 5 baris pertama sebagai output
print(df.head())

# tokenizing (bertujuan untuk memecah kalimat menjadi unit-unit kecil)

import nltk
nltk.download('punkt')
nltk.download('punkt_tab') # Download the punkt_tab resource
from nltk.tokenize import word_tokenize

# Fungsi untuk melakukan tokenizing
def tokenize_text(text):
    if pd.isna(text):
        return text
    tokens = word_tokenize(text)
    return " ".join(tokens)  # Join tokens back into a string

# Terapkan tokenizing ke semua kolom string
for col in df.columns:
    if df[col].dtype == 'object':
        df[col] = df[col].apply(tokenize_text)

# Simpan Data yang telah diubah
df.to_csv('/content/drive/My Drive/Tugas_akhir/DataTA(okky).csv', sep=';', index=False)

# Tampilkan 5 baris pertama sebagai output
print(df.head())

# stopwords removal

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

# Stopword removal
stop_words = set(stopwords.words('indonesian'))

def remove_stopwords(text):
    if pd.isna(text):
        return text
    words = text.split()
    filtered_words = [word for word in words if word not in stop_words]
    return " ".join(filtered_words)

# Apply stopword removal to all string columns
for col in df.columns:
    if df[col].dtype == 'object':
        df[col] = df[col].apply(remove_stopwords)

# Save the modified Data
df.to_csv('/content/drive/My Drive/Tugas_akhir/DataTA(okky).csv', sep=';', index=False)

# Display the first 5 rows as output
print(df.head())

# stemming pada data

!pip install Sastrawi

from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

# create stemmer
factory = StemmerFactory()
stemmer = factory.create_stemmer()

# Fungsi untuk melakukan stemming
def stem_text(text):
    if pd.isna(text):
        return text
    return stemmer.stem(text)

# Terapkan stemming ke semua kolom string
for col in df.columns:
    if df[col].dtype == 'object':
        df[col] = df[col].apply(stem_text)

# Simpan Data yang telah diubah
df.to_csv('/content/drive/My Drive/Tugas_akhir/DataTA(okky).csv', sep=';', index=False)

# Tampilkan 5 baris pertama sebagai output
print(df.head())

# hitung nilai tiap kolom (label) untuk HS_Religion, HS_Race, HS_Physical, HS_Gender, HS_Other

from google.colab import drive
import pandas as pd

drive.mount('/content/drive')

# Load the DataFrame
df = pd.read_csv('/content/drive/My Drive/Tugas_akhir/DataTA(okky).csv', sep=';')

# Define the columns to analyze
columns_to_analyze = ['HS_Religion', 'HS_Race', 'HS_Physical', 'HS_Gender', 'HS_Other']

# Calculate value counts for each specified column
for column in columns_to_analyze:
    print(f"\nValue counts for {column}:\n")
    print(df[column].value_counts())

# Hitung jumlah label 1 per baris di kolom-kolom label
df['label_sum'] = df[columns_to_analyze].sum(axis=1)

# Filter baris yang memiliki lebih dari satu label aktif
multilabel_df = df[df['label_sum'] > 1]

# Tampilkan hasil
print(f"\nJumlah komentar multilabel: {len(multilabel_df)}")
print("\nContoh komentar multilabel:\n")
print(multilabel_df[['Tweet'] + columns_to_analyze].head())  # tampilkan sebagian

import pandas as pd

# Load dataset
df = pd.read_csv('/content/drive/My Drive/Tugas_akhir/DataTA(okky).csv', sep=';')

# Kolom label yang tersedia
label_columns = ['HS_Religion', 'HS_Race', 'HS_Physical', 'HS_Gender', 'HS_Other']

# Filter hanya baris dengan 1 label aktif
df_filtered = df[df[label_columns].sum(axis=1) == 1].copy()

# Buat mapping baris ke label aktifnya (tanpa menambah ke DataFrame utama)
label_series = df_filtered[label_columns].idxmax(axis=1)

# Gabungkan DataFrame dengan label_series sementara
df_filtered['__label_temp__'] = label_series

# Ambil 30 sampel dari masing-masing kelas
test_df = df_filtered.groupby('__label_temp__').apply(lambda x: x.sample(n=30, random_state=42))
test_df.index = test_df.index.droplevel(0)

# Sisanya sebagai data latih
train_df = df_filtered.drop(index=test_df.index)

# Hapus kolom label sementara sebelum menyimpan
test_df.drop(columns='__label_temp__', inplace=True)
train_df.drop(columns='__label_temp__', inplace=True)

# Simpan ke CSV
train_df.to_csv('/content/drive/My Drive/Tugas_akhir/train_data_balanced.csv', sep=';', index=False)
test_df.to_csv('/content/drive/My Drive/Tugas_akhir/test_data_balanced.csv', sep=';', index=False)

# Info
print("Data training dan testing sudah disimpan tanpa kolom label tambahan.")
print("Jumlah data per kelas (sementara label):")
print(label_series[test_df.index].value_counts())

# splitting data

from google.colab import drive
import pandas as pd
from sklearn.model_selection import train_test_split

drive.mount('/content/drive')

# Load the preprocessed data
df = pd.read_csv('/content/drive/My Drive/Tugas_akhir/DataTA(okky).csv', sep=';')

# Split the data into training and testing sets (80% train, 20% test)
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42) #random_state for reproducibility

# Save the training and testing
train_df.to_csv('/content/drive/My Drive/Tugas_akhir/train_data.csv', sep=';', index=False)
test_df.to_csv('/content/drive/My Drive/Tugas_akhir/test_data.csv', sep=';', index=False)

print("Training data saved to /content/drive/My Drive/Tugas_akhir/train_data.csv")
print("Testing data saved to /content/drive/My Drive/Tugas_akhir/test_data.csv")