# -*- coding: utf-8 -*-
"""128.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lKi4Rxc2LGcQJi7RvXepsynXZIpr0IGy
"""

# !pip install fasttext

# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.id.300.vec.gz
# !gunzip cc.id.300.vec.gz

import os
import random
import numpy as np
import tensorflow as tf

SEED = 42
os.environ['PYTHONHASHSEED'] = str(SEED)
os.environ['TF_DETERMINISTIC_OPS'] = '1'  # Optional: agar operasi deterministik
random.seed(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)

import numpy as np
import numpy as np
import pandas as pd
import tensorflow as tf
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Bidirectional, LSTM, Dense, Dropout, Input
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Load dataset
train_data = pd.read_csv('/content/new_train_data.csv', sep=';')  # Data training
test_data = pd.read_csv('/content/new_test_data.csv', sep=';')  # Data testing

# Melihat jumlah data tiap label pada test_data
label_cols = ['HS_Religion', 'HS_Race', 'HS_Physical', 'HS_Gender', 'HS_Other']
label_counts = test_data[label_cols].sum().sort_values(ascending=False)

print("Jumlah data tiap label pada test_data:")
print(label_counts)

# melihat nilai nan pada dataset tsb

import pandas as pd

# Load the dataset
data = pd.read_csv('/content/new_train_data.csv', sep=';')

# Check for NaN values in each column
print(data.isnull().sum())

# Display rows with NaN values
print(data[data.isnull().any(axis=1)])

# Menyetel parameter untuk kamus embedding
MAX_VOCAB_SIZE = 50000 #kata yang paling sering muncul
MAX_SEQUENCE_LENGTH = 100 #menjaga agar input setara dan tidak terlalu panjang
EMBEDDING_DIM = 300
FASTTEXT_PATH = "/content/cc.id.300.vec"

# Tokenisasi teks
train_data['Tweet'] = train_data['Tweet'].astype(str)
test_data['Tweet'] = test_data['Tweet'].astype(str)

tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token="<OOV>")
tokenizer.fit_on_texts(train_data['Tweet'])
word_index = tokenizer.word_index

# Konversi teks menjadi urutan angka
X_train = pad_sequences(tokenizer.texts_to_sequences(train_data['Tweet']), maxlen=MAX_SEQUENCE_LENGTH, padding='post')
X_test = pad_sequences(tokenizer.texts_to_sequences(test_data['Tweet']), maxlen=MAX_SEQUENCE_LENGTH, padding='post')

# Mengubah label multi-class menjadi satu kolom kategori (0,1,2,...)
y_train = train_data[['HS_Religion', 'HS_Race', 'HS_Gender', 'HS_Physical', 'HS_Other']].idxmax(axis=1)
y_test = test_data[['HS_Religion', 'HS_Race', 'HS_Gender', 'HS_Physical', 'HS_Other']].idxmax(axis=1)

# Konversi kategori ke angka (misal: HS_Religion = 0, HS_Race = 1, dst.)
label_mapping = {label: idx for idx, label in enumerate(y_train.unique())}
y_train = y_train.map(label_mapping).values
y_test = y_test.map(label_mapping).values

# Tentukan jumlah kelas unik
num_classes = len(label_mapping)

# Konversi label ke dalam bentuk one-hot encoding
from tensorflow.keras.utils import to_categorical
y_train = to_categorical(y_train, num_classes=num_classes)
y_test = to_categorical(y_test, num_classes=num_classes)

# Load FastText embeddings
embedding_index = {}
with open(FASTTEXT_PATH, 'r', encoding='utf-8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        coefficients = np.asarray(values[1:], dtype='float32')
        embedding_index[word] = coefficients

# Membuat matriks embedding
embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))
for word, i in word_index.items():
    embedding_vector = embedding_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector

# Definisikan model
input_layer = Input(shape=(MAX_SEQUENCE_LENGTH,))
embedding_layer = Embedding(
    input_dim=len(word_index) + 1,
    output_dim=EMBEDDING_DIM,
    weights=[embedding_matrix],
    input_length=MAX_SEQUENCE_LENGTH,
    trainable=False
)(input_layer)

# CNN Layer
conv_layer = Conv1D(filters=64, kernel_size=2, activation='relu')(embedding_layer)
pool_layer = GlobalMaxPooling1D()(conv_layer)

# BiLSTM Layer
lstm_layer = Bidirectional(LSTM(64, return_sequences=False))(embedding_layer)

# Concatenate CNN and BiLSTM
concat_layer = tf.keras.layers.Concatenate()([pool_layer, lstm_layer])
dropout_layer = Dropout(0.5)(concat_layer)

# Output Layer
num_classes = len(label_mapping)  # Jumlah kelas unik dari label
output_layer = Dense(num_classes, activation='softmax')(dropout_layer)

# Compile model
model = Model(inputs=input_layer, outputs=output_layer)
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

# Model summary
model.summary()

# Train model
history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=30,
    batch_size=128,
    verbose=1
)

# Mendapatkan akurasi akhir
final_train_accuracy = history.history['accuracy'][-1]
final_val_accuracy = history.history['val_accuracy'][-1]

print(f"Akurasi Akhir (Training): {final_train_accuracy:.4f}")
print(f"Akurasi Akhir (Testing): {final_val_accuracy:.4f}")

import matplotlib.pyplot as plt

plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Test Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# evaluasi menggunakan confusion matrix
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true_classes = np.argmax(y_test, axis=1)

# Confusion Matrix
conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_mapping.keys(), yticklabels=label_mapping.keys())
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

# Classification Report
print("\nClassification Report:\n", classification_report(y_true_classes, y_pred_classes, target_names=label_mapping.keys()))

def predict_category_with_probs(text):
    # Preprocessing teks (tokenisasi dan padding)
    sequence = tokenizer.texts_to_sequences([text])
    padded_sequence = pad_sequences(sequence, maxlen=MAX_SEQUENCE_LENGTH, padding='post')

    # Prediksi model
    prediction = model.predict(padded_sequence)[0]  # ambil hasil prediksi pertama (karena input 1 teks)

    # Konversi indeks ke label
    label_mapping_inv = {idx: label for label, idx in label_mapping.items()}

    # Tampilkan semua skor prediksi per kategori
    print("Persentase Prediksi per Kategori:")
    for idx, prob in enumerate(prediction):
        label_name = label_mapping_inv[idx]
        print(f"  {label_name}: {prob * 100:.2f}%")

    # Ambil label dengan skor tertinggi
    predicted_idx = np.argmax(prediction)
    predicted_label = label_mapping_inv[predicted_idx]

    return predicted_label

# Contoh penggunaan fungsi prediksi
input_text = "Kafir nyamar. Pemakan babi. Mulut sama dengan junjungannya yang PK di tolak"
predicted_category = predict_category_with_probs(input_text)
print(f"\nKategori Prediksi Utama: {predicted_category}")

# # prompt: buat kode untuk bisa mendownload h5 dari model ini

# # Save the model in h5 format
# model.save('my_model.h5')

# # Download the saved model
# from google.colab import files
# files.download('my_model.h5')